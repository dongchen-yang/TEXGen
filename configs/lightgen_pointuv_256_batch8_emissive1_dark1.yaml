name: lightgen
tag: "pointuv_256res_batch8_emissive1_dark1"
exp_root_dir: "outputs"
seed: 42

# Data configuration - Full annotated dataset with 256x256 resolution
data_cls: spuv.data.lightgen_uv.LightGenDataModule
data:
  data_root: "../data/baked_uv_local"
  parquet_file: "../data/baked_uv_local/df_SomgProc_filtered.parquet"
  scene_list: ""
  eval_scene_list: ""
  repeat: 1
  vertex_transformation: false
  cond_views: 1
  sup_views: 4
  camera_strategy: "strategy_1"
  eval_cond_views: 1
  eval_sup_views: 4
  eval_camera_strategy: "strategy_test_1_to_4_90deg"

  # Train/Val/Test splits from annotated emissive data (filtered for local dataset)
  train_indices: "../data/baked_uv_local/data_splits_filtered.json"
  val_indices: "../data/baked_uv_local/data_splits_filtered.json"
  test_indices: "../data/baked_uv_local/data_splits_filtered.json"

  # Lower resolution settings for faster training and larger batch size
  height: 256
  width: 256
  uv_height: 256
  uv_width: 256
  batch_size: 8  # 4x larger batch size (memory scales ~O(n^2) with resolution)
  num_workers: 4

# System configuration
system_cls: spuv.systems.lightgen_system.LightGenSystem
system:
  condition_drop_rate: 0.1  # Enable dropout for regularization
  rescale_betas_zero_snr: true
  use_ema: true
  ema_decay: 0.9999
  val_with_ema: true
  render_background_color: [0.0, 0.0, 0.0]
  train_regression: false
  recon_warm_up_steps: 0
  prediction_type: "v_prediction"
  check_train_every_n_steps: 100
  test_save_json: false

  test_cfg_scale: 2.0
  test_num_steps: 50
  guidance_rescale: 0.0
  guidance_interval: [0.0, 1.0]

  cond_rgb_perturb: false

  # Image tokenizer (for CLIP conditioning)
  image_tokenizer_cls: spuv.models.tokenizers.clip.ClipTokenizer
  image_tokenizer:
    pretrained_model_name_or_path: "lambdalabs/sd-image-variations-diffusers"

  # Backbone: LightGenPointUVNet (modified PointUVNet for pre-baked UV data)
  backbone_cls: spuv.models.sparse_networks.lightgen_pointuvnet.LightGenPointUVNet
  backbone:
    # Input channels: noisy_emission(3) + position_map(3) + baked_texture(3) + baked_weights(1) = 10
    in_channels: 10
    out_channels: 3  # emission RGB
    
    # Architecture settings (same as TEXGen)
    num_layers: [1, 1, 1, 1, 1]
    point_block_num: [1, 1, 2, 4, 6]
    block_out_channels: [32, 256, 1024, 1024, 2048]
    dropout: [0.0, 0.0, 0.0, 0.1, 0.1]
    
    # Block types: mix of UV, hybrid point-UV, and attention-based
    block_type: ["uv", "point_uv", "uv_dit", "uv_dit", "uv_dit"]
    
    # Voxel sizes for 3D-aware grouping
    voxel_size: [0.01, 0.02, 0.05, 0.05, 0.05]
    
    # Window sizes for attention (adjusted for 256x256 resolution)
    # Original: [0, 256, 256, 512, 1024] for 512x512
    # For 256x256, we need to halve the window sizes to maintain the same receptive field ratio
    window_size: [0, 128, 128, 256, 256]
    
    # Number of attention heads
    num_heads: [4, 4, 16, 16, 16]
    
    # Skip connection settings
    skip_input: true
    skip_type: "adaptive"  # or "baked_texture" or "noise_input"
    
    use_uv_head: true

  # Loss configuration with emissive region loss AND dark region loss (balanced)
  loss:
    diffusion_loss_dict:
      lambda_mse: 1.0
      lambda_l1: 0.5
      lambda_emissive: 1.0  # Strong supervision on emissive regions (small area)
      lambda_dark_region: 0.25  # DARK LOSS: 0.25 - Moderate weight (large area)
      emissive_threshold: 0.001  # Threshold for detecting emissive regions
    render_loss_dict:
      lambda_render_lpips: 0.0
      lambda_render_mse: 0.0
      lambda_render_l1: 0.0
    use_min_snr_weight: false
    use_vgg: false

  # Optimizer (slightly higher LR for smaller resolution)
  optimizer:
    name: AdamW
    args:
      lr: 1.5e-4  # Slightly higher LR since we have larger effective batch size
      betas: [0.9, 0.999]
      weight_decay: 0.01

  # Learning rate scheduler
  scheduler:
    name: CosineAnnealingLR
    interval: step
    args:
      T_max: 100000
      eta_min: 1e-6

# Trainer configuration
trainer:
  max_epochs: 10
  check_val_every_n_epoch: 1  # Validate every epoch
  num_sanity_val_steps: 2
  precision: bf16-mixed
  gradient_clip_val: 1.0
  num_nodes: 1
  log_every_n_steps: 50
  # Note: accelerator and devices are set by launch.py based on --gpu flag

# Checkpoint configuration
checkpoint:
  save_last: true
  save_top_k: 5
  every_n_epochs: 1
  monitor: "val/psnr"
  mode: "max"
  save_on_train_epoch_end: false  # Wait for validation before saving


